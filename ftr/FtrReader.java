// FtrReader - Fast Transaction Recording reader implementation
package de.toem.impulse.extension.eda.transaction.ftr;

import java.io.IOException;
import java.io.InputStream;
import java.io.PushbackInputStream;
import java.nio.charset.StandardCharsets;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.HashSet;
import java.util.LinkedHashMap;
import java.util.LinkedHashSet;
import java.util.List;
import java.util.Map;

import de.toem.impulse.ImpulseBase;
import de.toem.impulse.cells.record.IRecord;
import de.toem.impulse.extension.eda.transaction.i18n.I18n;
import de.toem.impulse.samples.ISample;
import de.toem.impulse.samples.ISamplesReader;
import de.toem.impulse.samples.IStructSamplesWriter;
import de.toem.impulse.samples.domain.IDomainBase;
import de.toem.impulse.samples.domain.TimeBase;
import de.toem.impulse.samples.raw.StructMember;
import de.toem.impulse.serializer.AbstractSingleDomainRecordReader;
import de.toem.impulse.serializer.IParsingRecordReader;
import de.toem.toolkits.core.Utils;
import de.toem.toolkits.pattern.bundles.Bundles;
import de.toem.toolkits.pattern.element.ICell;
import de.toem.toolkits.pattern.element.serializer.ISerializerDescriptor;
import de.toem.toolkits.pattern.element.serializer.JavaSerializerPreference;
import de.toem.toolkits.pattern.element.serializer.SingletonSerializerPreference.DefaultSerializerConfiguration;
import de.toem.toolkits.pattern.filter.FilterExpression;
import de.toem.toolkits.pattern.ide.ConfiguredConsoleStream;
import de.toem.toolkits.pattern.ide.IConsoleStream;
import de.toem.toolkits.pattern.ide.Ide;
import de.toem.toolkits.pattern.pageable.BytesPageable;
import de.toem.toolkits.pattern.pageable.Pageable;
import de.toem.toolkits.pattern.properties.IPropertyModel;
import de.toem.toolkits.pattern.properties.PropertyModel;
import de.toem.toolkits.pattern.registry.IRegistryObject;
import de.toem.toolkits.pattern.registry.RegistryAnnotation;
import de.toem.toolkits.pattern.threading.IProgress;
import de.toem.toolkits.utils.serializer.ParseException;
import de.toem.toolkits.utils.text.MultilineText;
import kanzi.IndexedByteArray;
import kanzi.function.LZ4Codec;

/**
 * FTR (Fast Transaction Recording) reader implementation for impulse.
 *
 * This reader parses FTR format files and converts them into impulse record structures. The FTR format is a binary format that stores
 * transaction-based data, typically generated by hardware simulators, emulators, or other electronic design automation (EDA) tools.
 *
 * Key features implemented by this reader: - Parsing of CBOR-encoded FTR file format - Support for both compressed and uncompressed FTR sections -
 * Hierarchical signal creation from stream definitions - Configurable path separator for hierarchical signal names - Transaction event processing
 * with timestamp synchronization - Dictionary-based efficient string handling - Relation tracking between transactions - LZ4 decompression for
 * compressed data sections
 *
 * The reader processes several different section types within an FTR file: - Dictionary sections (tags 8-9): Store strings used throughout the file -
 * Directory sections (tags 10-11): Define streams and generators - Transaction chunks (tags 12-13): Contain the actual transaction data - Relations
 * sections (tags 14-15): Define relationships between transactions
 *
 * The hierarchical signal structure is created based on stream names, using a configurable path separator (default: ".") to determine the hierarchy.
 * Each component of the path becomes a scope in the record structure, with the final component being the signal name.
 *
 * Implementation Notes: - Uses TimeBase.ns (nanoseconds) as the base time unit for all signals - Preserves original stream hierarchies in the impulse
 * record structure - Adjusts timestamps based on the time scale from the file - Automatically detects files with .ftr extension
 *
 * Copyright (c) 2013-2025 Thomas Haber All rights reserved.
 *
 * @see de.toem.impulse.serializer.IParsingRecordReader
 */

@RegistryAnnotation(annotation = FtrReader.Annotation.class)
public class FtrReader extends AbstractSingleDomainRecordReader implements ISamplesReader.SignalProducer {

    public static class Annotation extends AbstractSingleDomainRecordReader.Annotation {

        public static final String id = "de.toem.impulse.reader.ftr";
        public static final String label = I18n.Serializer_FtrReader;
        public static final String description = I18n.Serializer_FtrReader_Description;
        public static final String helpURL = I18n.Serializer_FtrReader_HelpURL;
        public static final String defaultNamePattern = "\\.ftr$,\\.FTR$";
        public static final String formatType = "ftr";
        public static final String certificate = "EML/98HQHwifAPO8DAeUASUrBr5WzH78\nVhODxCEfY7ExnE2ylazpEwuuq2EVmdJT\ngxpkFOEmAqkU6uVBl8aJVVrYkwPSzJaF\n5pLcvVSqsASs32cE7YnIMRETtAegBG12\npEoaVokZbyfN8n+x6wMQ4CcGcxWOA9LY\nuIhjJH3o8OxpgsHjUp4vFR3QGmwOna0d\nETtv1pK8dv2TUx6u5nwdrPvIGJM5Dsvg\neH8G9ouw/NPBwk/PUS4m1y+hvE/6Q5MG\nn13+LqS8MYxKydQhO6Gawj70sUmND8k3\nImtR2zYLDDZThecFIUrspUAQPLxaWpQW\na7RbvOmPyFjFenZJxcaSD0edPrKfuOKA\n";
    }

    // ========================================================================================================================
    // Properties
    // ========================================================================================================================
    // Domain base used for time units
    private IDomainBase domainBase = TimeBase.ps;

    // Include signal filter list
    private List<FilterExpression> includeSignals;

    // Exclude signal filter list
    private List<FilterExpression> excludeSignals;

    // Start domain time filter
    private long startDomain = Long.MIN_VALUE;

    // End domain time filter
    private long endDomain = Long.MAX_VALUE;

    // Delay applied to domain times
    private long delayDomain = 0;

    // Scale applied to domain times
    private double scaleDomain = 1;

    // ========================================================================================================================
    // Content
    // ========================================================================================================================
    // Console stream for logging and output
    IConsoleStream console;

    // CBOR decoder for parsing the FTR file
    private CborDecoder cborDecoder;

    // Time scale factor for converting file time values to database time
    private long timeScaleFactor = 1000L;

    // List that stores string dictionary entries
    private final ArrayList<String> dictionary = new ArrayList<>();

    // Map to store streams: streamId -> Stream object
    private final Map<Long, Stream> streams = new HashMap<>();

    // Set to store handled lazy signals
    private final HashSet<IRecord.Signal> lazySignals = new HashSet<>();

    // Set to store lazy-loading signals
    private final HashSet<IRecord.Signal> lazyLoading = new LinkedHashSet<>();

    // Progress indicator for lazy loading
    private boolean parsing;

    // Constants for attribute data types
    // bool
    static final int ATTRIBUTE_BOOLEAN = 0;

    // enum
    static final int ATTRIBUTE_ENUMERATION = 1;

    // char, short, int, long
    static final int ATTRIBUTE_INTEGER = 2;

    // unsigned types
    static final int ATTRIBUTE_UNSIGNED = 3;

    // float, double
    static final int ATTRIBUTE_FLOATING_POINT_NUMBER = 4;

    // bit vectors
    static final int ATTRIBUTE_BIT_VECTOR = 5;

    // logic vectors
    static final int ATTRIBUTE_LOGIC_VECTOR = 6;

    // fixed point
    static final int ATTRIBUTE_FIXED_POINT_INTEGER = 7;

    // unsigned fixed point
    static final int ATTRIBUTE_UNSIGNED_FIXED_POINT_INTEGER = 8;

    // pointers
    static final int ATTRIBUTE_POINTER = 9;

    // string types
    static final int ATTRIBUTE_STRING = 10;

    // time
    static final int ATTRIBUTE_TIME = 11;

    // void type
    static final int ATTRIBUTE_NONE = 12;

    // CBOR tag constants
    // Self-describe CBOR tag (0xD9D9F7)
    static final long CBOR_SELF_DESCRIBE_TAG = 55799;

    // Epoch time tag
    static final long CBOR_EPOCH_TIME_TAG = 1;

    // File section tag constants
    // File info section tag
    static final int FILE_TAG_INFO = 6;

    // Uncompressed dictionary section
    static final int FILE_TAG_DICT_UNCOMPRESSED = 8;

    // Compressed dictionary section
    static final int FILE_TAG_DICT_COMPRESSED = 9;

    // Uncompressed directory section
    static final int FILE_TAG_DIR_UNCOMPRESSED = 10;

    // Compressed directory section
    static final int FILE_TAG_DIR_COMPRESSED = 11;

    // Uncompressed transaction chunk
    static final int FILE_TAG_TX_UNCOMPRESSED = 12;

    // Compressed transaction chunk
    static final int FILE_TAG_TX_COMPRESSED = 13;

    // Uncompressed relations section
    static final int FILE_TAG_REL_UNCOMPRESSED = 14;

    // Compressed relations section
    static final int FILE_TAG_REL_COMPRESSED = 15;

    // Directory entry tag constants
    // Stream definition tag
    static final int DIR_TAG_STREAM = 16;

    // Generator definition tag
    static final int DIR_TAG_GENERATOR = 17;

    // Transaction element tag constants
    // Transaction core information
    static final int TX_TAG_CORE = 6;

    // Begin attribute
    static final int TX_TAG_BEGIN_ATTR = 7;

    // Record attribute
    static final int TX_TAG_RECORD_ATTR = 8;

    // End attribute
    static final int TX_TAG_END_ATTR = 9;

    // Array length constants
    // Expected length for info arrays
    static final int EXPECTED_INFO_ARRAY_SIZE = 2;

    // Expected length for compressed dictionary arrays
    static final int EXPECTED_COMP_DICT_ARRAY_SIZE = 2;

    // Expected length for compressed directory arrays
    static final int EXPECTED_COMP_DIR_ARRAY_SIZE = 2;

    // Expected length for transaction arrays
    static final int EXPECTED_TX_ARRAY_SIZE = 4;

    // Expected length for compressed transaction arrays
    static final int EXPECTED_COMP_TX_ARRAY_SIZE = 5;

    // Expected length for stream definition arrays
    static final int EXPECTED_STREAM_DEF_ARRAY_SIZE = 3;

    // Expected length for generator definition arrays
    static final int EXPECTED_GENERATOR_DEF_ARRAY_SIZE = 3;

    // Expected length for transaction info arrays
    static final int EXPECTED_TX_INFO_ARRAY_SIZE = 4;

    // Expected length for attribute arrays
    static final int EXPECTED_ATTR_ARRAY_SIZE = 3;

    // Expected length for short relation arrays
    static final int EXPECTED_REL_ARRAY_SIZE_SHORT = 3;

    // Expected length for long relation arrays
    static final int EXPECTED_REL_ARRAY_SIZE_LONG = 5;

    // ========================================================================================================================
    // Constructors
    // ========================================================================================================================
    /**
     * Default constructor for the FtrReader.
     */
    public FtrReader() {
        super();
    }

    /**
     * Fully parameterized constructor for the FtrReader.
     *
     * @param descriptor
     *            The serializer descriptor providing contextual information
     * @param contentName
     *            The name of the content being processed
     * @param contentType
     *            The MIME type or other format descriptor of the content
     * @param cellType
     *            The type of cell that will be produced
     * @param configuration
     *            Configuration name for specialized settings
     * @param properties
     *            Additional properties as key-value pairs
     * @param in
     *            The input stream containing the data to be read
     */
    public FtrReader(ISerializerDescriptor descriptor, String contentName, String contentType, String cellType, String configuration,
            String[][] properties, InputStream in) {
        super(descriptor, configuration, properties, getPropertyModel(descriptor, null), in);
    }

    // ========================================================================================================================
    // Support Interface
    // ========================================================================================================================

    /**
     * Determines if this reader supports the specified functionality request.
     *
     * @param request
     *            An Integer identifying the functionality being queried
     * @param context
     *            Additional context for the request
     * @return true if the reader supports the requested functionality, false otherwise
     */
    public static boolean supports(Object request, Object context) {
        int ir = request instanceof Integer ? ((Integer) request).intValue() : -1;
        if (SUPPORT_CONFIGURATION == ir && DefaultSerializerConfiguration.TYPE.equals(context))
            return true;
        return ir == (ir & SUPPORT_PROPERTIES | SUPPORT_SOURCE);
    }

    /**
     * Create Java serializer preference cell for this reader.
     *
     * This factory method returns an ICell describing the Java preference for the serializer (used in UI/preferences). It configures label, help,
     * pattern and certificate and points to the implementation bundle.
     *
     * @return configured ICell instance for Java serializer preference
     */
    public static ICell createJavaPreference() {
        try {
            JavaSerializerPreference p = new JavaSerializerPreference();
            p.setName(Annotation.label);
            p.description = Annotation.description;
            p.helpUrl = Annotation.helpURL;
            p.namePattern = Annotation.defaultNamePattern;
            p.formatType = Annotation.formatType;
            p.certificate = Annotation.certificate;
            p.impl = MultilineText.toXml(Bundles.getBundleSourceEntryAsString(FtrReader.class));
            p.javaBundle = Utils.commarize(ImpulseBase.BUNDLE_ID,Bundles.getBundleId(FtrReader.class));     
            p.cellType = IRecord.Record.TYPE;
            return p;
        } catch (Throwable e) {
        }
        return null;
    }

    // ========================================================================================================================
    // Property Model
    // ========================================================================================================================
    /**
     * Creates and returns the property model for configuring this reader.
     *
     * @param object
     *            The serializer descriptor, used to provide context
     * @param context
     *            Additional context information
     * @return The property model containing all configurable properties for this reader
     */
    static public IPropertyModel getPropertyModel(ISerializerDescriptor object, Object context) {
        boolean notPref = context != IRegistryObject.Preference.class;
        PropertyModel model = IParsingRecordReader
                .getPropertyModel(PROP_INCLUDE | PROP_LAZY | PROP_HIERARCHY | (notPref ? (PROP_RANGE | PROP_TRANSFORM) : 0))
                .add(ConfiguredConsoleStream.getPropertyModel());
        model.setDefaultVal("hierarchy", ".");
        return model;
    }

    // ========================================================================================================================
    // Applicable
    // ========================================================================================================================
    /**
     * Determines if this reader can process the specified input based on the file name and content type.
     *
     * FTR files start with a CBOR self-describe tag (0xD9D9F7) followed by the indefinite-length array marker (0x9F). This method checks both the
     * file extension and the binary signature to properly identify valid FTR files.
     *
     * @param name
     *            The name of the file or content
     * @param contentType
     *            The MIME type or other format descriptor of the content
     * @param cellType
     *            The expected cell type to be produced
     * @param inputRequest
     *            Interface for examining the beginning of the input
     * @return APPLICABLE if this reader can process the input, NOT_APPLICABLE otherwise
     */
    @Override
    public int isApplicable(String name, String contentType, String cellType, IInputRequest inputRequest) {
        // First check file extension as a quick filter
        boolean hasExtension = name.toLowerCase().endsWith(".ftr");
        // Then verify the file header with the CBOR self-describe tag (55799 = 0xD9D9F7)
        // and the indefinite-length array marker (0x9F)
        try {
            byte[] header = inputRequest.bytes(4);
            if (header != null && header.length >= 4) {
                // Check for CBOR self-describe tag 55799 (0xD9D9F7) and indefinite array marker (0x9F)
                boolean hasValidHeader = (header[0] == (byte) 0xD9 && header[1] == (byte) 0xD9 && header[2] == (byte) 0xF7
                        && header[3] == (byte) 0x9F);
                if (hasValidHeader) {
                    return APPLICABLE;
                } else if (hasExtension) {
                    // Has correct extension but wrong header - might be corrupted or wrong format
                    return MAY_APPLICABLE;
                }
            }
        } catch (Exception e) {
            // If we can't read the header, fall back to extension check
            if (hasExtension) {
                return MAY_APPLICABLE;
            }
        }
        return NOT_APPLICABLE;
    }

    // ========================================================================================================================
    // Streams
    // ========================================================================================================================
    class Stream {

        // stream id
        long id;

        // Name of the stream
        String name;

        // Kind of stream (e.g. input, output)
        String kind;

        // Storage for compressed/uncompressed CBOR transaction chunks
        public Pageable<byte[]> chunks;

        // Map of generators by ID
        Map<Long, Generator> generators = new HashMap<>();

        // Reference to the impulse signal created for this stream
        IRecord.Signal signal;

        // Writer for structured samples associated with this stream
        IStructSamplesWriter writer;

        // Current time position in the stream
        long current;

        // Event queue for pending transactions
        class Event {

            Generator generator;

            long time;

            Event prev, next;

            int order;

            Map<String, StructMember> members = new LinkedHashMap<>();

            StructMember[] memberArray;

            Event(Generator generator) {
                this.generator = generator;
                members.put("__origin",
                        new StructMember(-1, null, "Origin", null, null, null, ISample.DATA_TYPE_ENUM, -1, ISample.FORMAT_DEFAULT, generator.name));
            }

            /**
             * Add a named StructMember to this event.
             *
             * @param name
             *            member name
             * @param member
             *            StructMember descriptor and value holder
             */
            void addMember(String name, StructMember member) {
                // Add a member to the event's members map
                members.put(name, member);
                memberArray = null;
            }

            /**
             * Return the StructMember array for this event.
             *
             * Builds the array from the internal map the first time it is requested.
             *
             * @return array of StructMember in insertion order
             */
            StructMember[] getMembers() {
                // Return the member array, creating it if necessary
                if (memberArray == null) {
                    memberArray = members.values().toArray(new StructMember[members.size()]);
                }
                return memberArray;
            }

            /**
             * Clean internal links and mark members invalid so the Event can be reused.
             *
             * Clears prev/next links and invalidates member validity flags.
             */
            void clean() {
                // clean up relation
                prev = next = null;
                // invalidate members
                if (memberArray != null) {
                    for (StructMember member : memberArray) {
                        member.setValid(false);
                    }
                }
            }
        }

        Event queueFirst;

        Event queueLast;

        class Generator {

            // Unique identifier for this generator
            long id;

            // Name of this generator
            String name;

            // Layer value for displaying transactions in UI
            int layer;

            // Current transaction identifier
            long txId;

            // Start time of current transaction
            long startTime;

            // End time of current transaction
            long endTime;

            // Events for begin and end attributes
            // begin/end
            Event[] events = new Event[2];

            // Reusable events for begin and end attributes
            // begin/end
            Event[] reuse = new Event[2];

            /**
             * Constructor for the Generator class.
             *
             * @param id
             *            The unique identifier for the generator
             * @param name
             *            The name of the generator
             * @param streamId
             *            The ID of the stream this generator belongs to
             */
            public Generator(long id, String name) {
                this.id = id;
                this.name = name;
                this.layer = Stream.this.generators.size();
            }

            /**
             * Begin a transaction on this generator.
             *
             * This prepares reusable Event objects for begin/end (or single) transactions, opens the writer if necessary and stores transaction
             * metadata.
             *
             * @param txId
             *            transaction identifier
             * @param startTime
             *            transaction start time (domain units)
             * @param endTime
             *            transaction end time (domain units), may equal startTime for single events
             * @throws ParseException
             *             on allocation/writer error
             */
            void begin(long txId, long startTime, long endTime) throws ParseException {

                // Create a new writer for this stream if it doesn't exist
                if (writer == null) {
                    writer = (IStructSamplesWriter) FtrReader.this.getWriter(Stream.this.signal);
                    writer.open(startTime);
                }
                // store transaction information
                this.txId = txId;
                this.startTime = startTime;
                this.endTime = endTime;
                // Provide Events - reuse if available
                if (reuse[0] != null) {
                    events[0] = reuse[0];
                    reuse[0] = reuse[0].next;
                    events[0].clean();
                } else
                    events[0] = new Event(this);
                events[0].time = startTime;
                if (endTime > startTime) {
                    if (reuse[1] != null) {
                        events[1] = reuse[1];
                        reuse[1] = reuse[1].next;
                        events[1].clean();
                    } else
                        events[1] = new Event(this);
                    events[1].time = endTime;
                    events[0].order = ISample.GO_INITIAL;
                    events[1].order = ISample.GO_FINAL;
                } else {
                    events[0].order = ISample.GO_SINGLE;
                    events[1] = null;
                }
            }

            /**
             * Set an attribute value for the current transaction.
             *
             * The attribute is assigned to either the begin or end Event depending on tag. New StructMember is created when the attribute name hasn't
             * been seen before.
             *
             * @param tag
             *            element tag (begin/record/end)
             * @param name
             *            attribute name
             * @param value
             *            attribute value
             * @param attrType
             *            attribute type constant
             * @throws ParseException
             *             on invalid attribute type
             */
            void set(int tag, String name, Object value, int attrType) throws ParseException {
                if (name == null || value == null) {
                    return;
                }
                // Check if attribute already exists
                Event e = events[1] != null && tag != TX_TAG_BEGIN_ATTR ? events[1] : events[0];
                StructMember member = e.members.get(name);
                if (member == null) {
                    e.memberArray = null;
                    // Create new attribute based on attribute type
                    // Use auto-incrementing ID for struct members
                    // Use name as description by default
                    String description = null;
                    // No specific icon
                    String iconId = null;
                    // No specific tags
                    String tags = null;
                    // Use default scale
                    int scale = ISample.SCALE_DEFAULT;
                    // Will be overridden for specific types
                    String format = ISample.FORMAT_DEFAULT;
                    switch (attrType) {
                    case ATTRIBUTE_BOOLEAN:
                        member = new StructMember(-1, null, name, description, iconId, tags, ISample.DATA_TYPE_ENUM, scale, ISample.FORMAT_BOOLEAN,
                                value);
                        break;
                    case ATTRIBUTE_ENUMERATION:
                        member = new StructMember(-1, null, name, description, iconId, tags, ISample.DATA_TYPE_ENUM, scale, ISample.FORMAT_DEFAULT,
                                value);
                        break;
                    case ATTRIBUTE_INTEGER:
                        member = new StructMember(-1, null, name, description, iconId, tags, ISample.DATA_TYPE_INTEGER, scale, ISample.FORMAT_DECIMAL,
                                value);
                        break;
                    case ATTRIBUTE_UNSIGNED:
                        member = new StructMember(-1, null, name, description, iconId, tags, ISample.DATA_TYPE_INTEGER, scale, ISample.FORMAT_DECIMAL,
                                value);
                        break;
                    case ATTRIBUTE_POINTER:
                        member = new StructMember(-1, null, name, description, iconId, tags, ISample.DATA_TYPE_INTEGER, scale,
                                ISample.FORMAT_HEXADECIMAL, value);
                        break;
                    case ATTRIBUTE_TIME:
                        member = new StructMember(-1, null, name, description, iconId, tags, ISample.DATA_TYPE_INTEGER, scale, ISample.FORMAT_DEFAULT,
                                value);
                        break;
                    case ATTRIBUTE_FLOATING_POINT_NUMBER:
                    case ATTRIBUTE_FIXED_POINT_INTEGER:
                    case ATTRIBUTE_UNSIGNED_FIXED_POINT_INTEGER:
                        member = new StructMember(-1, null, name, description, iconId, tags, ISample.DATA_TYPE_FLOAT, scale, ISample.FORMAT_DEFAULT,
                                value);
                        break;
                    case ATTRIBUTE_STRING:
                        member = new StructMember(-1, null, name, description, iconId, tags, /* ISample.DATA_TYPE_TEXT */
                                ISample.DATA_TYPE_ENUM, scale, ISample.FORMAT_DEFAULT, value);
                        break;
                    case ATTRIBUTE_BIT_VECTOR:
                        member = new StructMember(-1, null, name, description, iconId, tags, ISample.DATA_TYPE_LOGIC, scale, ISample.FORMAT_BINARY,
                                value);
                        break;
                    case ATTRIBUTE_LOGIC_VECTOR:
                        member = new StructMember(-1, null, name, description, iconId, tags, ISample.DATA_TYPE_LOGIC, scale, ISample.FORMAT_DEFAULT,
                                value);
                        break;
                    default:
                        // For unknown attribute types, create a text member as fallback
                        throw new ParseException("Missing attribute type for " + name);
                    }
                    // Add the new attribute to the attributes map
                    e.addMember(name, member);
                }
                member.setValue(value);
                member.setValid(true);
            }

            /**
             * Finalize the current transaction and insert prepared events into the stream queue.
             *
             * @throws ParseException
             *             on queue insertion failure
             */
            void end() throws ParseException {
                insertIntoQueue(events[0]);
                if (events[1] != null) {
                    // If there is an end event, insert it into the queue
                    insertIntoQueue(events[1]);
                }
            }

            /**
             * Write a prepared Event to the stream writer and recycle the Event for reuse.
             *
             * @param event
             *            event to write
             * @throws ParseException
             *             on writer error
             */
            void write(Event event) throws ParseException {
                // write the event to the structured samples writer
                writer.write(event.time, 0, event.order, layer, event.getMembers());
                // Clean up the event before reusing it
                event.clean();
                // store for reuse
                int n = event.order == ISample.GO_FINAL ? 1 : 0;
                Event e = reuse[n];
                reuse[n] = event;
                event.next = e;
            }
        }

        /**
         * Constructor for the Stream class.
         *
         * @param id
         *            The unique identifier for the stream
         * @param name
         *            The name of the stream
         * @param kind
         *            The kind of the stream (e.g., "input", "output")
         */
        public Stream(long id, String name, String kind) {
            this.id = id;
            this.name = name;
            this.kind = kind;
            initSignal();
        }

        void initSignal() {
            // Get path separator from properties (default to '.' if not specified)
            String separator = FtrReader.this.getProperty("hierarchy");
            // Split signal name into hierarchical components using the separator
            String[] pathComponents = name.split(java.util.regex.Pattern.quote(separator));
            // Determine the parent scope and signal name based on path components
            IRecord.Scope parentScope = null;
            String signalName = name;
            // If we have hierarchical components, process them
            if (pathComponents.length > 1) {
                // Create the scope path for parent scopes
                StringBuilder scopePath = new StringBuilder();
                for (int i = 0; i < pathComponents.length - 1; i++) {
                    if (i > 0) {
                        scopePath.append('/');
                    }
                    scopePath.append(pathComponents[i]);
                }
                // Ensure all parent scopes exist
                parentScope = FtrReader.this.assertScope(scopePath.toString());
                // Last component becomes the signal name
                signalName = pathComponents[pathComponents.length - 1];
            }
            // Create the signal with the appropriate parent scope and name
            this.signal = FtrReader.this.addSignal(parentScope, signalName, kind, null, ISample.DATA_TYPE_STRUCT, -1, ISample.FORMAT_DEFAULT,
                    domainBase);
        }

        /**
         * Add a generator definition to this stream.
         *
         * @param id
         *            generator id
         * @param name
         *            generator name
         */
        public void addGenerator(long id, String name) {
            generators.put(id, new Generator(id, name));
        }

        /**
         * Insert an event into the stream's time-ordered queue.
         *
         * The insertion keeps the queue sorted by event.time. It typically walks backwards from the tail as recent events are most common insertion
         * points.
         *
         * @param Event
         *            event to be inserted into the queue
         */
        void insertIntoQueue(Event Event) {
            // Maintain a time-ordered double-linked queue.
            // We typically insert near the tail (most recent events) to be efficient,
            // walking backwards until we find the proper insertion point.
            if (queueFirst == null) {
                queueFirst = queueLast = Event;
            } else {
                // sort into the queue based on time, start from last
                Event current = queueLast;
                while (current != null && current.time > Event.time) {
                    current = current.prev;
                }
                if (current == null) {
                    // Insert at the beginning
                    Event.next = queueFirst;
                    queueFirst.prev = Event;
                    queueFirst = Event;
                } else if (current == queueLast) {
                    // Insert at the end (common case)
                    current.next = Event;
                    Event.prev = current;
                    queueLast = Event;
                } else {
                    // Insert in the middle: relink neighbors
                    Event.prev = current;
                    Event.next = current.next;
                    current.next.prev = Event;
                    current.next = Event;
                }
            }
        }

        /**
         * Remove and return the first (earliest) event from the queue.
         *
         * @return the earliest Event or null if queue is empty
         */
        Event removeFirstFromQueue() {
            if (queueFirst == null) {
                return null;
            }
            Event found = queueFirst;
            queueFirst = found.next;
            if (queueFirst != null) {
                queueFirst.prev = null;
            } else {
                // If the queue is now empty, reset last
                queueLast = null;
            }
            return found;
        }

        /**
         * Flush and write queued events up to the provided upper time bound.
         *
         * Events with event.time <= untilTime are written in chronological order.
         *
         * @param untilTime
         *            upper time bound in domain units
         * @throws ParseException
         *             on writer error
         */
        void finish(long untilTime) throws ParseException {
            // Pop events from the queue in chronological order and write them
            Event event = null;
            while ((event = removeFirstFromQueue()) != null && event.time <= untilTime) {
                event.generator.write(event);
            }
        }

        /**
         * Add a transaction chunk to this stream for later processing.
         *
         * In lazy mode the chunk is stored; in eager mode it will be parsed immediately.
         *
         * @param chunkData
         *            CBOR chunk bytes
         * @param startTime
         *            start time associated with this chunk
         * @param endTime
         *            end time associated with this chunk
         */
        public void addChunk(byte[] chunkData, long startTime, long endTime) {
            // Initialize the chunks pageable if it doesn't exist
            if (chunks == null)
                chunks = new BytesPageable();
            chunks.set(chunks.addFragment(), chunkData);
        }

        /**
         * Close the stream writer and release resources.
         *
         * Flushes remaining events and closes the writer, updating the signal metadata.
         *
         * @throws ParseException
         *             on writer error
         */
        public void close() throws ParseException {
            if (writer != null) {
                finish(Long.MAX_VALUE);
                // Close the writer if it exists
                writer.close(current + 1);
                signal.update(writer);
                writer = null;
            }
        }

        @Override
        public String toString() {
            return "Stream{id=" + id + ", name='" + name + "', kind='" + kind + "'}";
        }
    }

    // ========================================================================================================================
    // Lazy reading
    // ========================================================================================================================

    /**
     * Indicates if reader operates in lazy mode.
     *
     * @return true when lazy loading is enabled via properties
     */
    @Override
    public boolean isLazy() {
        return getTypedProperty("lazy", Boolean.class) == Boolean.TRUE;
    }

    /**
     * Notify the reader that a signal should be produced lazily.
     *
     * If parsing is complete and lazy mode is enabled the signal is scheduled for deferred parsing and population.
     *
     * @param signal
     *            the signal to produce (IRecord.Signal)
     */
    @Override
    public void produceSignal(IRecord.Signal signal) {
        // If parsing is in progress, we cannot produce lazy signals yet
        if (!parsing && isLazy() && !lazySignals.contains(signal)) {
            // If lazy mode is enabled and the signal is not already in the lazy signals set
            lazySignals.add(signal);
            console.info("produceSignal called for signal: ", signal);
            synchronized (lazyLoading) {
                lazyLoading.add(signal);
            }
            // creates a thread to handle parsing
            parseLazy(p -> parseStreams(p));
        }
    }

    /**
     * Parses lazy loading streams in a separate thread. This method is called when lazy loading is enabled and signals are produced.
     *
     * @param runnable
     *            The runnable to execute for parsing streams
     */
    void parseStreams(IProgress p) {
        // This method is called to produce all signals in lazy mode
        console.info("produceSignals called, processing lazy signals");
        long started = Utils.millies();
        while (!p.isCanceled()) {
            IRecord.Signal signal = null;
            synchronized (lazyLoading) {
                // Get the next lazy loading stream if available
                if (!lazyLoading.isEmpty()) {
                    // Get the first stream from the lazy loading set
                    signal = lazyLoading.iterator().next();
                    lazyLoading.remove(signal);
                }
            }
            if (signal == null) {
                // No more lazy loading signals to process
                console.info("No more lazy loading signals to process");
                break;
            }
            // find stream with signal
            Stream stream = null;
            if (signal != null)
                for (Stream s : streams.values()) {
                    if (s.signal.equals(signal)) {
                        stream = s;
                        break;
                    }
                }
            if (stream == null) {
                console.error("No stream found for signal: ", signal.getName());
                continue;
            }
            console.info("Processing lazy loading stream: ", stream.name);
            try {
                // get the chunks and call parseTxChunk for each
                if (stream.chunks != null) {
                    console.log("Processing ", stream.chunks.getFragmentCount(), " chunks  ");
                    for (int i = 0; i < stream.chunks.getFragmentCount(); i++) {
                        if (p.isCanceled())
                            break;
                        byte[] chunkData = stream.chunks.get(i);
                        if (chunkData != null && chunkData.length > 0) {
                            console.log("Parsing chunk ", i, " of size ", chunkData.length);
                            parseTxChunk(new CborDecoder(chunkData), stream, stream.current, stream.current + chunkData.length);
                            flushAndSetProgress(progress);
                        }
                    }
                } else {
                    console.log("No chunks to process for signal: ", signal.getName());
                }
            } catch (ParseException e) {
                console.error("Error parsing transaction chunk for signal " + signal.getName() + ": " + e.getMessage());
            } finally {
                try {
                    stream.close();
                } catch (ParseException e) {
                    console.error("Error closing stream: " + e.getMessage());
                }
            }
        }
        console.major("Used time: ", (Utils.millies() - started), " ms");
    }

    // ========================================================================================================================
    // Parser
    // ====================================================================================================================
    /**
     * Parses the input stream and creates a record structure. This implementation follows the FTR format specification.
     *
     * @param progress
     *            Interface for reporting progress and checking for cancellation
     * @param in
     *            The input stream containing the data to be read
     * @throws ParseException
     *             If an error occurs during parsing
     */
    @Override
    protected void parse(IProgress progress, InputStream in) throws ParseException {
        // Set up console for logging and initialize parsing state
        long current = 0;
        console = new ConfiguredConsoleStream(Ide.DEFAULT_CONSOLE, ConfiguredConsoleStream.logging(getProperties()));
        console.info("Starting FTR file parsing xx");
        long started = Utils.millies();
        parsing = true;
        try {
            // Parse properties and range settings
            // range
            this.startDomain = domainBase.parseMultiple(getProperty("start"), IDomainBase.PARSE_BIG | IDomainBase.PARSE_DOMAINBASE, this.startDomain)
                    .longValue();
            this.endDomain = domainBase.parseMultiple(getProperty("end"), IDomainBase.PARSE_BIG | IDomainBase.PARSE_DOMAINBASE, this.endDomain)
                    .longValue();
            this.delayDomain = domainBase.parseMultiple(getProperty("delay"), IDomainBase.PARSE_BIG | IDomainBase.PARSE_DOMAINBASE, this.delayDomain)
                    .longValue();
            this.scaleDomain = Utils.parseDouble(getProperty("scale"), this.scaleDomain);
            // exclude/include
            this.includeSignals = FilterExpression.createList(getProperty("include"), FilterExpression.TYPE_REGULAR | FilterExpression.TYPE_TEXT);
            this.excludeSignals = FilterExpression.createList(getProperty("excludes"), FilterExpression.TYPE_REGULAR | FilterExpression.TYPE_TEXT);

            // Create CBOR decoder and initialize record structure
            cborDecoder = new CborDecoder(in);
            // Create empty record structure (we'll add signals later)
            initRecord("FTR Record", domainBase);
            changed(CHANGED_RECORD);
            console.info("Starting FTR parseInput");
            // Read the initial CBOR tag which should be 55799 (self-describe CBOR)
            long cborTag = cborDecoder.readTag();
            console.info("CBOR initial tag: ", cborTag);
            if (cborTag != CBOR_SELF_DESCRIBE_TAG) {
                console.error("Invalid CBOR tag: ", cborTag);
                throw new ParseException("Not a valid FTR file. Expected CBOR tag " + CBOR_SELF_DESCRIBE_TAG + ", found: " + cborTag);
            }
            // Read the indefinite-length array
            long arrayLen = cborDecoder.readArrayLength();
            console.info("Root array length: ", arrayLen);
            if (arrayLen != -1) {
                console.error("Invalid array length: ", arrayLen);
                throw new ParseException("Expected indefinite-length array, found fixed length: " + arrayLen);
            }
            CborType next = cborDecoder.peekType();
            CborType breakType = CborType.valueOf(0xff);
            int sectionCount = 0;
            // Main section parsing loop
            while ((progress == null || !progress.isCanceled()) && next != null && !breakType.isEqualType(next)) {
                long tag = cborDecoder.readTag();
                sectionCount++;
                console.info("Processing section #", sectionCount, " with tag: ", tag);
                switch ((int) tag) {
                case FILE_TAG_INFO: {
                    // Parse the INFO section: time scale and epoch
                    console.info("Found INFO section (tag " + FILE_TAG_INFO + ")");
                    CborDecoder cbd = new CborDecoder(cborDecoder.readByteString());
                    long sz = cbd.readArrayLength();
                    if (sz != EXPECTED_INFO_ARRAY_SIZE) {
                        console.error("Invalid info array size: ", sz);
                        throw new ParseException("Invalid info section size: " + sz);
                    }
                    long timeScale = cbd.readInt();
                    // Assuming we're using nanoseconds (scale -9) as our base
                    // Database time scale
                    long effectiveTimeScale = timeScale - (-9);
                    timeScaleFactor = calculateTimescaleMultiplier(effectiveTimeScale);
                    console.info("Time scale: ", timeScale, " effective scale: ", effectiveTimeScale, " multiplier: ", timeScaleFactor);
                    long epochTag = cbd.readTag();
                    if (epochTag != CBOR_EPOCH_TIME_TAG) {
                        console.error("Invalid epoch tag: ", epochTag);
                        throw new ParseException("Expected epoch tag " + CBOR_EPOCH_TIME_TAG + ", found: " + epochTag);
                    }
                    // epoch time
                    long epoch = cbd.readInt();
                    console.info("Epoch time: ", epoch);
                    break;
                }
                case FILE_TAG_DICT_UNCOMPRESSED: {
                    // Parse uncompressed dictionary section
                    console.info("Found UNCOMPRESSED DICTIONARY section (tag " + FILE_TAG_DICT_UNCOMPRESSED + ")");
                    byte[] dictBytes = cborDecoder.readByteString();
                    console.info("Dictionary size: ", dictBytes.length, " bytes");
                    parseDict(new CborDecoder(dictBytes));
                    console.info("Dictionary parsed successfully, entries: ", dictionary.size());
                    break;
                }
                case FILE_TAG_DICT_COMPRESSED: {
                    // Parse compressed dictionary section
                    console.info("Found COMPRESSED DICTIONARY section (tag " + FILE_TAG_DICT_COMPRESSED + ")");
                    long sz = cborDecoder.readArrayLength();
                    if (sz != EXPECTED_COMP_DICT_ARRAY_SIZE) {
                        console.error("Invalid compressed dictionary array size: ", sz);
                        throw new ParseException("Invalid compressed dictionary section size: " + sz);
                    }
                    long uncompressedSize = cborDecoder.readInt();
                    console.info("Dictionary uncompressed size: ", uncompressedSize);
                    byte[] compressedDict = cborDecoder.readByteString();
                    console.info("Dictionary compressed size: ", compressedDict.length, " bytes");
                    parseDict(new CborDecoder(decompressLZ4(compressedDict, uncompressedSize)));
                    console.info("Compressed dictionary parsed successfully, entries: ", dictionary.size());
                    break;
                }
                case FILE_TAG_DIR_UNCOMPRESSED: {
                    // Parse uncompressed directory section
                    console.info("Found UNCOMPRESSED DIRECTORY section (tag " + FILE_TAG_DIR_UNCOMPRESSED + ")");
                    byte[] dirBytes = cborDecoder.readByteString();
                    console.info("Directory size: ", dirBytes.length, " bytes");
                    parseDir(new CborDecoder(dirBytes));
                    console.info("Directory parsed successfully");
                    break;
                }
                case FILE_TAG_DIR_COMPRESSED: {
                    // Parse compressed directory section
                    console.info("Found COMPRESSED DIRECTORY section (tag " + FILE_TAG_DIR_COMPRESSED + ")");
                    long sz = cborDecoder.readArrayLength();
                    if (sz != EXPECTED_COMP_DIR_ARRAY_SIZE) {
                        console.error("Invalid compressed directory array size: ", sz);
                        throw new ParseException("Invalid compressed directory section size: " + sz);
                    }
                    long uncompressedSize = cborDecoder.readInt();
                    console.info("Directory uncompressed size: ", uncompressedSize);
                    byte[] compressedDir = cborDecoder.readByteString();
                    console.info("Directory compressed size: ", compressedDir.length, " bytes");
                    parseDir(new CborDecoder(decompressLZ4(compressedDir, uncompressedSize)));
                    console.info("Compressed directory parsed successfully");
                    break;
                }
                case FILE_TAG_TX_UNCOMPRESSED: {
                    // Parse uncompressed transaction chunk
                    console.info("Found UNCOMPRESSED TRANSACTION CHUNK section (tag " + FILE_TAG_TX_UNCOMPRESSED + ")");
                    long len = cborDecoder.readArrayLength();
                    if (len != EXPECTED_TX_ARRAY_SIZE) {
                        console.error("Invalid transaction chunk array size: ", len);
                        throw new ParseException("Invalid transaction chunk size: " + len);
                    }
                    long streamId = cborDecoder.readInt();
                    long startTime = cborDecoder.readInt() * timeScaleFactor;
                    long endTime = cborDecoder.readInt() * timeScaleFactor;
                    console.info("Transaction chunk for stream ID: ", streamId, " time range: ", startTime, endTime);
                    // Process transaction chunk data
                    byte[] chunkData = cborDecoder.readByteString();
                    console.info("Transaction chunk size: ", chunkData.length, " bytes");
                    Stream stream = streams.get(streamId);
                    if (stream != null) {
                        if (isLazy())
                            stream.addChunk(chunkData, startTime, endTime);
                        else
                            parseTxChunk(new CborDecoder(chunkData), stream, startTime, endTime);
                        console.info("Transaction chunk parsed successfully");
                    }
                    break;
                }
                case FILE_TAG_TX_COMPRESSED: {
                    // Parse compressed transaction chunk
                    console.info("Found COMPRESSED TRANSACTION CHUNK section (tag " + FILE_TAG_TX_COMPRESSED + ")");
                    long len = cborDecoder.readArrayLength();
                    if (len != EXPECTED_COMP_TX_ARRAY_SIZE) {
                        console.error("Invalid compressed transaction chunk array size: ", len);
                        throw new ParseException("Invalid compressed transaction chunk size: " + len);
                    }
                    long streamId = cborDecoder.readInt();
                    long startTime = cborDecoder.readInt() * timeScaleFactor;
                    long endTime = cborDecoder.readInt() * timeScaleFactor;
                    long uncompressedSize = cborDecoder.readInt();
                    console.info("Compressed transaction chunk for stream ID: ", streamId, ", time range: ", startTime, " - ", endTime,
                            ", uncompressed size: ", uncompressedSize);
                    // Process compressed transaction chunk data
                    byte[] compressedData = cborDecoder.readByteString();
                    console.info("Compressed transaction chunk size: ", compressedData.length, " bytes");
                    Stream stream = streams.get(streamId);
                    if (stream != null) {
                        if (isLazy())
                            stream.addChunk(decompressLZ4(compressedData, uncompressedSize), startTime, endTime);
                        else
                            parseTxChunk(new CborDecoder(decompressLZ4(compressedData, uncompressedSize)), stream, startTime, endTime);
                        console.info("Compressed transaction chunk parsed successfully");
                    }
                    break;
                }
                case FILE_TAG_REL_UNCOMPRESSED: {
                    // Parse uncompressed relations section
                    console.info("Found UNCOMPRESSED RELATIONS section (tag " + FILE_TAG_REL_UNCOMPRESSED + ")");
                    byte[] relBytes = cborDecoder.readByteString();
                    console.info("Relations size: ", relBytes.length, " bytes");
                    parseRel(new CborDecoder(relBytes));
                    console.info("Relations parsed successfully");
                    break;
                }
                case FILE_TAG_REL_COMPRESSED: {
                    // Parse compressed relations section
                    console.info("Found COMPRESSED RELATIONS section (tag " + FILE_TAG_REL_COMPRESSED + ")");
                    long sz = cborDecoder.readArrayLength();
                    if (sz != EXPECTED_COMP_DICT_ARRAY_SIZE) {
                        console.error("Invalid compressed relations array size: ", sz);
                        throw new ParseException("Invalid compressed relations section size: " + sz);
                    }
                    long uncompressedSize = cborDecoder.readInt();
                    console.info("Relations uncompressed size: ", uncompressedSize);
                    byte[] compressedRel = cborDecoder.readByteString();
                    console.info("Relations compressed size: ", compressedRel.length, " bytes");
                    parseRel(new CborDecoder(decompressLZ4(compressedRel, uncompressedSize)));
                    console.info("Compressed relations parsed successfully");
                    break;
                }
                default:
                    // Unknown tag encountered
                    console.warning("Unknown tag in FTR file: ", tag);
                    throw new ParseException("Unknown tag in FTR file: " + tag);
                }
                next = cborDecoder.peekType();
                flushAndSetProgress(progress);
            }
            console.info("FTR parsing complete. Total sections processed: ", sectionCount);
        } catch (Exception e) {
            // Handle and wrap exceptions
            if (!(e instanceof ParseException))
                e = new ParseException("Failed to parse FTR file: " + e.getMessage(), e);
            console.error("Error during FTR parsing: ", e);
            throw (ParseException) e;
        } finally {
            // Close streams and log timing
            if (!isLazy())
                for (Stream stream : streams.values())
                    stream.close();
            console.major("Used time: ", (Utils.millies() - started), " ms");
            parsing = false;
            // Ensure resources are released
            try {
                if (in != null)
                    in.close();
            } catch (Exception e) {
                // Ignore exceptions on close
            }
        }
    }

    /**
     * Calculate time scale multiplier based on the power of 10
     *
     * @param power
     *            Power of 10 to calculate
     * @return The multiplier (10^power)
     */
    private static long calculateTimescaleMultiplier(long power) {
        if (power <= 0) {
            return 1;
        }
        // Use Math.pow which is more efficient for large powers
        return (long) Math.pow(10, power);
    }

    /**
     * Parse string dictionary section.
     *
     * Reads a CBOR map mapping indices to text strings and appends them to the internal dictionary.
     *
     * @param decoder
     *            CBOR decoder positioned at the dictionary map
     * @throws ParseException
     *             on I/O or format error
     */
    private void parseDict(CborDecoder decoder) throws ParseException {
        // Reads a CBOR map of index->string and adds to the dictionary list
        try {
            long size = decoder.readMapLength();
            ArrayList<String> list = new ArrayList<>((int) size);
            for (long i = 0; i < size; ++i) {
                long idx = decoder.readInt();
                if (idx != dictionary.size() + list.size()) {
                    throw new ParseException("Dictionary index mismatch. Expected: " + (dictionary.size() + list.size()) + ", found: " + idx);
                }
                list.add(decoder.readTextString());
            }
            dictionary.addAll(list);
        } catch (IOException e) {
            throw new ParseException("Failed to parse dictionary: " + e.getMessage(), e);
        }
    }

    /**
     * Parse directory section containing stream and generator definitions.
     *
     * @param decoder
     *            CBOR decoder positioned at the directory array
     * @throws ParseException
     *             on I/O or format error
     */
    private void parseDir(CborDecoder decoder) throws ParseException {
        // Reads a CBOR array of stream/generator definitions and adds them to the model
        try {
            long size = decoder.readArrayLength();
            CborType breakType = CborType.valueOf(0xff);
            if (size < 0) {
                // Indefinite-length array
                CborType next = decoder.peekType();
                while (next != null && !breakType.isEqualType(next)) {
                    parseDirEntry(decoder);
                    next = decoder.peekType();
                }
            } else {
                // Fixed-length array
                for (long i = 0; i < size; ++i) {
                    parseDirEntry(decoder);
                }
            }
            FtrReader.this.changed(CHANGED_RECORD);
        } catch (IOException e) {
            throw new ParseException("Failed to parse directory: " + e.getMessage(), e);
        }
    }

    /**
     * Parse a single directory entry (stream or generator).
     *
     * @param decoder
     *            CBOR decoder positioned at a directory entry tag
     * @throws ParseException
     *             on format error
     */
    private void parseDirEntry(CborDecoder decoder) throws ParseException {
        // Reads a stream or generator definition and adds it to the model
        try {
            long id = decoder.readTag();
            if (id == DIR_TAG_STREAM) {
                // Stream definition
                long len = decoder.readArrayLength();
                if (len != EXPECTED_STREAM_DEF_ARRAY_SIZE) {
                    throw new ParseException(
                            "Invalid stream definition, expected array length " + EXPECTED_STREAM_DEF_ARRAY_SIZE + ", found: " + len);
                }
                long streamId = decoder.readInt();
                long nameId = decoder.readInt();
                long kindId = decoder.readInt();
                // Create signal for this stream
                String streamName = dictionary.get((int) nameId);
                String streamKind = dictionary.get((int) kindId);
                console.info("Adding stream with ID: ", streamId, ", name: ", streamName, ", kind: ", streamKind);
                streams.put(streamId, new Stream(streamId, streamName, streamKind));
            } else if (id == DIR_TAG_GENERATOR) {
                // Generator definition
                long len = decoder.readArrayLength();
                if (len != EXPECTED_GENERATOR_DEF_ARRAY_SIZE) {
                    throw new ParseException(
                            "Invalid generator definition, expected array length " + EXPECTED_GENERATOR_DEF_ARRAY_SIZE + ", found: " + len);
                }
                long genId = decoder.readInt();
                long nameId = decoder.readInt();
                long streamId = decoder.readInt();
                // Create generator signal
                String genName = dictionary.get((int) nameId);
                if (!streams.containsKey(streamId)) {
                    throw new ParseException("Generator references unknown stream ID: " + streamId);
                }
                console.info("Adding generator with ID: ", genId, ", name: ", genName, ", stream ID: ", streamId);
                streams.get(streamId).addGenerator(genId, genName);
            } else {
                throw new ParseException("Invalid directory entry tag: " + id);
            }
        } catch (IOException e) {
            throw new ParseException("Failed to parse directory entry: " + e.getMessage(), e);
        }
    }

    /**
     * Parse transaction chunk data for a specific stream.
     *
     * Parses indefinite-length CBOR arrays of transactions. Each transaction is a definite-length array of tagged elements. The method dispatches
     * tags to core transaction handling and attribute setting.
     *
     * @param decoder
     *            CBOR decoder for the chunk payload
     * @param stream
     *            target Stream instance
     * @param startTime
     *            chunk start time (domain units)
     * @param endTime
     *            chunk end time (domain units)
     * @throws ParseException
     *             on format or I/O error
     */
    private void parseTxChunk(CborDecoder decoder, Stream stream, long startTime, long endTime) throws ParseException {
        // Parses a CBOR indefinite-length array of transactions for a stream
        console.info("Parsing transaction chunk for stream ID: ", stream.id, " time range: ", startTime, endTime);
        try {
            // Read outer indefinite array
            long size = decoder.readArrayLength();
            if (size != -1) {
                throw new ParseException("Expected indefinite-length array in transaction chunk, got: " + size);
            }
            console.info("Reading indefinite-length transaction array");
            int txCount = 0;
            // Process transactions until break is found
            while (true) {
                int initialByte = decoder.readRawByte();
                if (initialByte == 0xff) {
                    // Break type
                    console.info("End of transaction chunk reached, processed ", txCount, " transactions");
                    break;
                }
                if (initialByte == -1) {
                    throw new ParseException("Unexpected end of stream");
                }
                decoder.unreadByte(initialByte);
                // Each transaction is a definite-length array
                long txArrayLen = decoder.readArrayLength();
                if (txArrayLen < 0) {
                    throw new ParseException("Expected definite-length array for transaction, got: " + txArrayLen);
                }
                console.log("Transaction #", txCount + 1, " with ", txArrayLen, " elements");
                txCount++;
                Stream.Generator generator = null;
                for (long i = 0; i < txArrayLen; i++) {
                    // Each element starts with a tag
                    long tag = decoder.readTag();
                    console.log("  Transaction element #", i, " with tag: ", tag);
                    switch ((int) tag) {
                    case TX_TAG_CORE: {
                        // Transaction core info: txId, genId, start, end
                        long len = decoder.readArrayLength();
                        if (len != EXPECTED_TX_INFO_ARRAY_SIZE) {
                            throw new ParseException("Invalid transaction info array length: " + len);
                        }
                        long txId = decoder.readInt();
                        long genId = decoder.readInt();
                        long txStartTime = decoder.readInt() * timeScaleFactor;
                        long txEndTime = decoder.readInt() * timeScaleFactor;
                        // Get generator for this transaction
                        generator = stream.generators.get(genId);
                        if (generator != null)
                            generator.begin(txId, txStartTime, txEndTime);
                        else
                            throw new ParseException("Generator ID " + genId + " not found in stream " + stream.id);
                        console.log("  Transaction core: ID=", txId, " genID=", genId, " time= ", txStartTime, txEndTime);
                        break;
                    }
                    // Begin attribute, record attribute, or end attribute
                    case TX_TAG_BEGIN_ATTR:
                    case TX_TAG_RECORD_ATTR:
                    case TX_TAG_END_ATTR: {
                        // Attribute: name, type, value
                        long len = decoder.readArrayLength();
                        if (len != EXPECTED_ATTR_ARRAY_SIZE) {
                            throw new ParseException("Invalid attribute array length: " + len);
                        }
                        long nameId = decoder.readInt();
                        long typeId = decoder.readInt();
                        String attrName = dictionary.get((int) nameId);
                        console.log("  Attribute: name=", attrName, " type=", typeId);
                        Object val = null;
                        switch ((int) typeId) {
                        case ATTRIBUTE_BOOLEAN:
                            val = decoder.readBoolean();
                            break;
                        case ATTRIBUTE_INTEGER:
                        case ATTRIBUTE_UNSIGNED:
                        case ATTRIBUTE_POINTER:
                        case ATTRIBUTE_TIME:
                            val = decoder.readInt();
                            break;
                        case ATTRIBUTE_FLOATING_POINT_NUMBER:
                        case ATTRIBUTE_FIXED_POINT_INTEGER:
                        case ATTRIBUTE_UNSIGNED_FIXED_POINT_INTEGER:
                            val = decoder.readFloat();
                            break;
                        case ATTRIBUTE_ENUMERATION:
                        case ATTRIBUTE_BIT_VECTOR:
                        case ATTRIBUTE_LOGIC_VECTOR:
                        case ATTRIBUTE_STRING:
                            val = dictionary.get((int) decoder.readInt());
                            break;
                        default:
                            throw new ParseException("Unexpected typeId in transaction: " + typeId);
                        }
                        if (generator != null)
                            generator.set((int) tag, attrName, val, (int) typeId);
                    }
                        break;
                    default:
                        throw new ParseException("Unexpected tag in transaction: " + tag);
                    }
                }
                if (generator != null)
                    generator.end();
            }
            stream.finish(endTime);
            FtrReader.this.changed(CHANGED_SIGNALS);
        } catch (IOException e) {
            throw new ParseException("Failed to parse transaction chunk: " + e.getMessage(), e);
        }
    }

    /**
     * Parse relations section containing arrays of relation descriptors.
     *
     * Supports both short and long relation array shapes (with or without fiber info).
     *
     * @param decoder
     *            CBOR decoder positioned at the relations array
     * @throws ParseException
     *             on format or I/O error
     */
    private void parseRel(CborDecoder decoder) throws ParseException {
        // Parses a CBOR indefinite-length array of relations
        try {
            CborType breakType = CborType.valueOf(0xff);
            long size = decoder.readArrayLength();
            if (size != -1) {
                throw new ParseException("Expected indefinite-length array for relations");
            }
            CborType next = decoder.peekType();
            while (next != null && !breakType.isEqualType(next)) {
                long sz = decoder.readArrayLength();
                if (sz != EXPECTED_REL_ARRAY_SIZE_LONG && sz != EXPECTED_REL_ARRAY_SIZE_SHORT) {
                    throw new ParseException("Invalid relation structure, expected array length " + EXPECTED_REL_ARRAY_SIZE_LONG + " or "
                            + EXPECTED_REL_ARRAY_SIZE_SHORT + ", found: " + sz);
                }
                long typeId = decoder.readInt();
                long fromId = decoder.readInt();
                long toId = decoder.readInt();
                // Read fiber information if present
                if (sz > EXPECTED_REL_ARRAY_SIZE_SHORT) {
                    long fromFiber = decoder.readInt();
                    long toFiber = decoder.readInt();
                    // Store relation with fiber info
                    createRelation(typeId, fromId, toId, fromFiber, toFiber);
                } else {
                    // Store relation without fiber info
                    createRelation(typeId, fromId, toId);
                }
                next = decoder.peekType();
            }
        } catch (IOException e) {
            throw new ParseException("Failed to parse relations: " + e.getMessage(), e);
        }
    }

    /**
     * Create a relation between transactions
     *
     * @param typeId
     *            Type ID of the relation
     * @param fromId
     *            Source transaction ID
     * @param toId
     *            Target transaction ID
     */
    private void createRelation(long typeId, long fromId, long toId) {
        createRelation(typeId, fromId, toId, -1, -1);
    }

    /**
     * Create a relation between transactions with fiber information
     *
     * @param typeId
     *            Type ID of the relation
     * @param fromId
     *            Source transaction ID
     * @param toId
     *            Target transaction ID
     * @param fromFiber
     *            Source fiber
     * @param toFiber
     *            Target fiber
     */
    private void createRelation(long typeId, long fromId, long toId, long fromFiber, long toFiber) {
        // In a real implementation, this would create a relation in the impulse record
        // Placeholder implementation - to be completed based on impulse API requirements
    }

    /**
     * Helper method to decompress LZ4 data and return the decompressed byte array
     *
     * @param compressedData
     *            The LZ4 compressed data
     * @param uncompressedSize
     *            The expected uncompressed size
     * @return byte array containing the decompressed data
     * @throws ParseException
     *             If decompression fails
     */
    private byte[] decompressLZ4(byte[] compressedData, long uncompressedSize) throws ParseException {
        // Decompresses LZ4-compressed data using the kanzi LZ4Codec
        try {
            console.info("Decompressing ", compressedData.length, " bytes (expected uncompressed size: ", uncompressedSize, ")");
            // Create the codec for decompression
            LZ4Codec codec = new LZ4Codec();
            // Allocate a buffer with the expected uncompressed size
            // Add a 10% safety margin in case the size estimate is slightly off
            int bufferSize = Math.max((int) uncompressedSize + (int) (uncompressedSize / 10), 65536);
            console.info("Using buffer size: ", bufferSize, " bytes");
            byte[] output = new byte[bufferSize];
            // Setup source and destination arrays
            IndexedByteArray source = new IndexedByteArray(compressedData, 0);
            IndexedByteArray destination = new IndexedByteArray(output, 0);
            // Single-shot decompression
            if (!codec.inverse(source, destination)) {
                throw new ParseException("LZ4 decompression failed");
            }
            // Create a properly sized array with just the decompressed data
            byte[] result = new byte[destination.index];
            System.arraycopy(output, 0, result, 0, destination.index);
            console.info("Successfully decompressed ", compressedData.length, " bytes to ", destination.index, " bytes");
            return result;
        } catch (Exception e) {
            throw new ParseException("Error during LZ4 decompression: " + e.getMessage(), e);
        }
    }
}

// ========================================================================================================================
// CBOR Decoder
// ========================================================================================================================
/**
 * CBOR Constants - constant values used by the CBOR format.
 */
class CborConstants {

    // Major type 0: unsigned integers.
    static final int TYPE_UNSIGNED_INTEGER = 0x00;

    // Major type 1: negative integers.
    static final int TYPE_NEGATIVE_INTEGER = 0x01;

    // Major type 2: byte string.
    static final int TYPE_BYTE_STRING = 0x02;

    // Major type 3: text/UTF8 string.
    static final int TYPE_TEXT_STRING = 0x03;

    // Major type 4: array of items.
    static final int TYPE_ARRAY = 0x04;

    // Major type 5: map of pairs.
    static final int TYPE_MAP = 0x05;

    // Major type 6: semantic tags.
    static final int TYPE_TAG = 0x06;

    // Major type 7: floating point, simple data types.
    static final int TYPE_FLOAT_SIMPLE = 0x07;

    // Denotes a one-byte value (uint8).
    static final int ONE_BYTE = 0x18;

    // Denotes a two-byte value (uint16).
    static final int TWO_BYTES = 0x19;

    // Denotes a four-byte value (uint32).
    static final int FOUR_BYTES = 0x1a;

    // Denotes a eight-byte value (uint64).
    static final int EIGHT_BYTES = 0x1b;

    // The CBOR-encoded boolean false value.
    static final int FALSE = 0x14;

    // The CBOR-encoded boolean true value.
    static final int TRUE = 0x15;

    // The CBOR-encoded null value.
    static final int NULL = 0x16;

    // The CBOR-encoded "undefined" value.
    static final int UNDEFINED = 0x17;

    // Denotes a half-precision float (two-byte IEEE 754)
    static final int HALF_PRECISION_FLOAT = 0x19;

    // Denotes a single-precision float (four-byte IEEE 754)
    static final int SINGLE_PRECISION_FLOAT = 0x1a;

    // Denotes a double-precision float (eight-byte IEEE 754)
    static final int DOUBLE_PRECISION_FLOAT = 0x1b;

    // The CBOR-encoded "break" stop code for unlimited arrays/maps.
    static final int BREAK = 0x1f;

    // Semantic tag values
    static final int TAG_STANDARD_DATE_TIME = 0;

    static final int TAG_EPOCH_DATE_TIME = 1;

    static final int TAG_POSITIVE_BIGINT = 2;

    static final int TAG_NEGATIVE_BIGINT = 3;

    static final int TAG_DECIMAL_FRACTION = 4;

    static final int TAG_CBOR_ENCODED = 24;
}

/**
 * CborType - Represents the various major types in CBOR
 */
class CborType {

    private final int major;

    private final int additional;

    private CborType(int major, int additional) {
        this.major = major;
        this.additional = additional;
    }

    /**
     * Returns a descriptive string for the given major type.
     */
    public static String getName(int mt) {
        // Handle break type (0xff) as a special case
        if ((mt & 0xff) == 0xff) {
            return "break";
        }
        // Get only the major type bits
        int majorType = mt & 0x07;
        switch (majorType) {
        case CborConstants.TYPE_ARRAY:
            return "array";
        case CborConstants.TYPE_BYTE_STRING:
            return "byte string";
        case CborConstants.TYPE_FLOAT_SIMPLE:
            return "float/simple value";
        case CborConstants.TYPE_MAP:
            return "map";
        case CborConstants.TYPE_NEGATIVE_INTEGER:
            return "negative integer";
        case CborConstants.TYPE_TAG:
            return "tag";
        case CborConstants.TYPE_TEXT_STRING:
            return "text string";
        case CborConstants.TYPE_UNSIGNED_INTEGER:
            return "unsigned integer";
        default:
            throw new IllegalArgumentException("Invalid major type: " + mt);
        }
    }

    /**
     * Decodes a given byte value to a {@link CborType} value.
     */
    public static CborType valueOf(int i) {
        // Handle break type (0xff) specially to avoid major type calculation issues
        if ((i & 0xff) == 0xff) {
            return new CborType(CborConstants.TYPE_FLOAT_SIMPLE, CborConstants.BREAK);
        }
        return new CborType((i & 0xff) >>> 5, i & 0x1f);
    }

    /**
     * @return the major type, as integer value from [0..7].
     */
    public int getMajorType() {
        return major;
    }

    /**
     * @return the additional information of this type, as integer value from [0..31].
     */
    public int getAdditionalInfo() {
        return additional;
    }

    /**
     * @return <code>true</code> if this type allows for an infinite-length payload
     */
    public boolean isBreakAllowed() {
        return major == CborConstants.TYPE_ARRAY || major == CborConstants.TYPE_BYTE_STRING || major == CborConstants.TYPE_MAP
                || major == CborConstants.TYPE_TEXT_STRING;
    }

    /**
     * Determines whether the major type of a given {@link CborType} equals the major type of this one
     */
    public boolean isEqualType(CborType other) {
        if (other == null) {
            throw new IllegalArgumentException("Parameter cannot be null!");
        }
        return major == other.major;
    }

    /**
     * Determines whether the major type of a given byte value equals the major type of this one
     */
    public boolean isEqualType(int encoded) {
        return major == ((encoded & 0xff) >>> 5);
    }

    @Override
    public String toString() {
        return getName(major) + "(" + additional + ")";
    }
}

class CborDecoder {

    // ---------------------------------------------------------------------
    // Fields / construction
    // ---------------------------------------------------------------------
    private final PushbackInputStream inputStream;

    private byte[] buffer;

    private int pos = 0;

    private int bufferSize = 0;

    // 64KB maximum chunk size
    private static final int MAX_CHUNK_SIZE = 65536;

    private boolean endOfInput = false;

    // Flag to indicate if we're using a direct byte array
    private boolean directBuffer = false;

    /**
     * Creates a new decoder that reads data in chunks from the input stream.
     */
    public CborDecoder(InputStream is) throws IOException {
        if (is == null)
            throw new IllegalArgumentException("InputStream cannot be null!");
        this.inputStream = is instanceof PushbackInputStream ? (PushbackInputStream) is : new PushbackInputStream(is, 1);
        this.buffer = new byte[MAX_CHUNK_SIZE];
        this.directBuffer = false;
        // Initialize with first chunk of data
        readMoreData();
    }

    /**
     * Creates a new decoder that reads data directly from a byte array. This constructor is more efficient when all data is already available.
     *
     * @param data
     *            The byte array containing CBOR data
     */
    public CborDecoder(byte[] data) {
        if (data == null)
            throw new IllegalArgumentException("Data byte array cannot be null!");
        // No input stream is needed
        this.inputStream = null;
        this.buffer = data;
        this.bufferSize = data.length;
        // We're using the array directly
        this.directBuffer = true;
        this.pos = 0;
    }

    /**
     * Attempts to read more data from the input stream into the buffer. Returns true if data was read, false if end of stream was reached. When using
     * a direct byte array (directBuffer=true), this method returns false as no more data needs to be read.
     */
    private boolean readMoreData() throws IOException {
        // Fill buffer when using a streaming input. If directBuffer (byte[] provided) is used,
        // reading more is not necessary. When data remains in buffer move it to start to free space.
        if (directBuffer || endOfInput)
            return false;
        // If there's data left in the buffer, move it to the beginning
        if (pos > 0 && pos < bufferSize) {
            System.arraycopy(buffer, pos, buffer, 0, bufferSize - pos);
            bufferSize -= pos;
            pos = 0;
        } else if (pos >= bufferSize) {
            // Reset buffer if all data has been consumed
            bufferSize = 0;
            pos = 0;
        }
        // Read more data into the buffer
        int bytesRead = inputStream.read(buffer, bufferSize, buffer.length - bufferSize);
        if (bytesRead == -1) {
            endOfInput = true;
            return false;
        }
        bufferSize += bytesRead;
        return true;
    }

    // ---------------------------------------------------------------------
    // Position helpers
    // ---------------------------------------------------------------------
    /**
     * Return the current read position (byte offset) inside the internal buffer.
     *
     * @return current byte position
     */
    public long getPos() {
        return pos;
    }

    /**
     * Ensures that at least n bytes are available in the buffer. Tries to read more data if needed and fails if not enough data is available.
     *
     * @param n
     *            Number of bytes needed
     * @throws IOException
     *             if an I/O error occurs or if there's not enough data
     */
    private void ensureData(long n) throws IOException {
        // Ensure 'n' bytes available in buffer; try to read more chunks if needed.
        if (pos + n <= bufferSize)
            return;
        // Try to read more data
        while (pos + n > bufferSize && readMoreData())
            ;
        // Fail if we still don't have enough data
        if (pos + n > bufferSize) {
            // If still not enough data, this is an unexpected EOF
            fail("Unexpected end of input, needed " + n + " more bytes");
        }
    }

    /**
     * Read a single byte from the buffer, advancing the position.
     *
     * Ensures at least one byte is available, reading more from the underlying stream if necessary.
     *
     * @return unsigned byte value in range [0..255]
     * @throws IOException
     *             on I/O or EOF
     */
    private int readByte() throws IOException {
        ensureData(1);
        return buffer[pos++] & 0xff;
    }

    /**
     * Unread the last consumed byte (move position back by one) or push it back to the stream when at the buffer beginning.
     *
     * Note: this method assumes the previously returned byte is the one to unread.
     */
    private void unreadByte() {
        if (pos == 0) {
            try {
                // If we're at the start of the buffer, use the PushbackInputStream
                inputStream.unread(buffer[0] & 0xff);
            } catch (IOException e) {
                throw new IllegalStateException("Cannot unread byte", e);
            }
        } else {
            // Simply move back one position in our buffer
            --pos;
        }
    }

    // ---------------------------------------------------------------------
    // Public highlevel API
    // ---------------------------------------------------------------------

    /**
     * Look at, but do not consume, the next major type.
     */
    public CborType peekType() throws IOException {
        if (pos >= bufferSize && !readMoreData())
            return null;
        int b = readByte();
        unreadByte();
        return CborType.valueOf(b);
    }

    /**
     * Read the length of an array major type (returns -1 for indefinite-length).
     *
     * @return length of the array or -1 for indefinite-length arrays
     * @throws IOException
     *             on I/O or format error
     */
    public long readArrayLength() throws IOException {
        return readMajorTypeWithSize(CborConstants.TYPE_ARRAY);
    }

    /**
     * Read the length of a map major type (returns -1 for indefinite-length).
     *
     * @return length of the map or -1 for indefinite-length maps
     * @throws IOException
     *             on I/O or format error
     */
    public long readMapLength() throws IOException {
        return readMajorTypeWithSize(CborConstants.TYPE_MAP);
    }

    /**
     * Read a text (UTF-8) string value.
     *
     * Supports only definite-length strings; indefinite-length text strings are rejected.
     *
     * @return decoded Java String
     * @throws IOException
     *             on I/O or format error
     */
    public String readTextString() throws IOException {
        long len = readMajorTypeWithSize(CborConstants.TYPE_TEXT_STRING);
        if (len < 0)
            fail("Indefinitelength text not supported");
        ensureData(len);
        String s = new String(buffer, pos, (int) len, StandardCharsets.UTF_8);
        pos += (int) len;
        return s;
    }

    /**
     * Read a byte string (raw bytes).
     *
     * Supports only definite-length byte strings; indefinite-length byte strings are rejected.
     *
     * @return byte[] with the requested bytes
     * @throws IOException
     *             on I/O or format error
     */
    public byte[] readByteString() throws IOException {
        long len = readMajorTypeWithSize(CborConstants.TYPE_BYTE_STRING);
        if (len < 0)
            fail("Indefinitelength byte string not supported");
        ensureData(len);
        // Use Arrays.copyOfRange for cleaner code and potentially better optimization
        byte[] out = Arrays.copyOfRange(buffer, pos, pos + (int) len);
        pos += (int) len;
        return out;
    }

    /**
     * Read a semantic tag value.
     *
     * @return tag value as unsigned long
     * @throws IOException
     *             on I/O or format error
     */
    public long readTag() throws IOException {
        return readUInt(readMajorType(CborConstants.TYPE_TAG), false);
    }

    /**
     * Read an integer value (positive or negative).
     *
     * This is a fast path that verifies major type and dispatches to readUInt.
     *
     * @return decoded integer (negative integers returned as negative numbers)
     * @throws IOException
     *             on I/O or format error
     */
    public long readInt() throws IOException {
        int ib = readByte();
        int major = (ib >>> 5) & 0x07;
        int ai = ib & 0x1f;
        if (major != CborConstants.TYPE_UNSIGNED_INTEGER && major != CborConstants.TYPE_NEGATIVE_INTEGER)
            fail("Expected integer major type, got %s", CborType.getName(major));
        long n = readUInt(ai, false);
        return (major == CborConstants.TYPE_UNSIGNED_INTEGER) ? n : -1L - n;
    }

    /**
     * Read a CBOR boolean value.
     *
     * @return true or false
     * @throws IOException
     *             on I/O or format error
     */
    public boolean readBoolean() throws IOException {
        int ib = readByte();
        int major = (ib >>> 5) & 0x07;
        if (major != CborConstants.TYPE_FLOAT_SIMPLE)
            fail("Expected simple value, got %s", CborType.getName(major));
        int ai = ib & 0x1f;
        if (ai == CborConstants.TRUE)
            return true;
        if (ai == CborConstants.FALSE)
            return false;
        fail("Additionalinfo %d is not a boolean", ai);
        // unreachable
        return false;
    }

    /**
     * Read a floating-point value (half/single/double) or integer coerced to float.
     *
     * @return float value
     * @throws IOException
     *             on I/O or format error
     */
    public float readFloat() throws IOException {
        int ib = readByte();
        int major = (ib >>> 5) & 0x07;
        int ai = ib & 0x1f;
        if (major == CborConstants.TYPE_FLOAT_SIMPLE) {
            switch (ai) {
            case CborConstants.HALF_PRECISION_FLOAT:
                return halfToFloat(readUInt16());
            case CborConstants.SINGLE_PRECISION_FLOAT:
                return Float.intBitsToFloat((int) readUInt32());
            case CborConstants.DOUBLE_PRECISION_FLOAT:
                return (float) Double.longBitsToDouble(readUInt64());
            default:
                fail("Unexpected additionalinfo %d for float", ai);
            }
        } else if (major == CborConstants.TYPE_UNSIGNED_INTEGER || major == CborConstants.TYPE_NEGATIVE_INTEGER) {
            unreadByte();
            return (float) readInt();
        }
        fail("Expected float/int, got %s", CborType.getName(major));
        // unreachable
        return 0f;
    }

    /**
     * Read a raw byte from the stream (unsigned).
     *
     * @return byte value in [0..255]
     * @throws IOException
     *             on I/O or format error
     */
    public int readRawByte() throws IOException {
        return readByte();
    }

    /**
     * Unread a raw byte value (compatibility API).
     *
     * This simply calls the internal unreadByte() helper.
     *
     * @param b
     *            ignored (kept for API compatibility)
     */
    public void unreadByte(int b) {
        unreadByte();
    }

    // ---------------------------------------------------------------------
    // Lowlevel helpers
    // ---------------------------------------------------------------------

    /**
     * Signal a parsing failure by throwing an IOException with position info.
     *
     * @param fmt
     *            message format
     * @param args
     *            format args
     * @throws IOException
     *             always
     */
    private void fail(String fmt, Object... args) throws IOException {
        throw new IOException(String.format(fmt, args) + " @pos=" + getPos());
    }

    /**
     * Read and validate major type byte, returning the additional-info field.
     *
     * @param expectedMajor
     *            expected major type constant
     * @return additional-info value
     * @throws IOException
     *             on I/O or unexpected major type
     */
    private int readMajorType(int expectedMajor) throws IOException {
        int ib = readByte();
        int major = (ib >>> 5) & 0x07;
        if (major != expectedMajor)
            fail("Unexpected major %s, expected %s", CborType.getName(major), CborType.getName(expectedMajor));
        return ib & 0x1f;
    }

    /**
     * Read major type and interpret size information, supporting indefinite-length.
     *
     * @param major
     *            expected major type constant
     * @return length or -1 for indefinite-length
     * @throws IOException
     *             on I/O or format error
     */
    private long readMajorTypeWithSize(int major) throws IOException {
        return readUInt(readMajorType(major), true);
    }

    /**
     * Decode an unsigned integer value from additional-info and following bytes.
     *
     * Supports the CBOR "break" additional-info (returns -1 when breakAllowed==true).
     *
     * @param ai
     *            additional-info nibble
     * @param breakAllowed
     *            permit break code to return -1
     * @return decoded unsigned integer or -1 if break encountered and allowed
     * @throws IOException
     *             on I/O or format error
     */
    private long readUInt(int ai, boolean breakAllowed) throws IOException {
        if (ai < CborConstants.ONE_BYTE) {
            return ai;
        }
        switch (ai) {
        case CborConstants.ONE_BYTE:
            return readByte();
        case CborConstants.TWO_BYTES:
            return readUInt16();
        case CborConstants.FOUR_BYTES:
            return readUInt32();
        case CborConstants.EIGHT_BYTES:
            return readUInt64();
        case CborConstants.BREAK:
            if (breakAllowed)
                return -1;
        }
        fail("Illegal additionalinfo value %d", ai);
        // unreachable
        return 0;
    }

    /**
     * Read a 16-bit unsigned integer.
     *
     * @return value in range [0..65535]
     * @throws IOException
     *             on I/O or EOF
     */
    private int readUInt16() throws IOException {
        ensureData(2);
        // Combine operations to reduce variable assignments
        int result = ((buffer[pos] & 0xff) << 8) | (buffer[pos + 1] & 0xff);
        pos += 2;
        return result;
    }

    /**
     * Read a 32-bit unsigned integer and return as long.
     *
     * @return unsigned 32-bit value as long
     * @throws IOException
     *             on I/O or EOF
     */
    private long readUInt32() throws IOException {
        // Read 4 bytes as unsigned 32-bit integer; use local vars for faster JIT optimization
        ensureData(4);
        int p = pos;
        byte[] buf = buffer;
        long result = ((buf[p] & 0xffL) << 24) | ((buf[p + 1] & 0xffL) << 16) | ((buf[p + 2] & 0xffL) << 8) | (buf[p + 3] & 0xffL);
        pos = p + 4;
        return result & 0xffffffffL;
    }

    /**
     * Read a 64-bit unsigned integer.
     *
     * @return 64-bit value
     * @throws IOException
     *             on I/O or EOF
     */
    private long readUInt64() throws IOException {
        // Read 8 bytes as unsigned 64-bit integer; same optimization pattern as readUInt32
        ensureData(8);
        int p = pos;
        byte[] buf = buffer;
        long result = ((buf[p] & 0xffL) << 56) | ((buf[p + 1] & 0xffL) << 48) | ((buf[p + 2] & 0xffL) << 40) | ((buf[p + 3] & 0xffL) << 32)
                | ((buf[p + 4] & 0xffL) << 24) | ((buf[p + 5] & 0xffL) << 16) | ((buf[p + 6] & 0xffL) << 8) | (buf[p + 7] & 0xffL);
        pos = p + 8;
        return result;
    }

    /**
     * Convert a CBOR half-precision float (16-bit) to Java float.
     *
     * Handles zero, subnormal, normal, infinity and NaN cases.
     *
     * @param bits
     *            16-bit half float bits
     * @return 32-bit IEEE float value
     */
    private static float halfToFloat(int bits) {
        // Convert 16-bit IEEE754-half to 32-bit float.
        // Handles zero, subnormal, normalized and special cases (Inf/NaN).
        int s = (bits >>> 15) & 0x1;
        int e = (bits >>> 10) & 0x1f;
        int f = bits & 0x3ff;
        if (e == 0) {
            // 0 or subnormal numbers
            if (f == 0)
                return s == 1 ? -0.0f : 0.0f;
            return (float) ((s == 1 ? -1 : 1) * f * Math.pow(2, -24)); // subnormal scaling
        }
        if (e == 31) {
            // infinity or NaN
            return f == 0 ? (s == 1 ? Float.NEGATIVE_INFINITY : Float.POSITIVE_INFINITY) : Float.NaN;
        }
        // Normalized value
        return (float) ((s == 1 ? -1 : 1) * (1 + f / 1024.0) * Math.pow(2, e - 15));
    }
}